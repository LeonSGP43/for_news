import { GoogleGenAI, type ThinkingLevel } from '@google/genai'

let ai: GoogleGenAI | null = null

function getAI(): GoogleGenAI {
  if (!ai) {
    const apiKey = process.env.GEMINI_API_KEY
    if (!apiKey) {
      throw new Error('GEMINI_API_KEY is not set')
    }
    console.log('ğŸ¤– Initializing Gemini AI...')
    ai = new GoogleGenAI({ apiKey })
  }
  return ai
}

// æ·±åº¦åˆ†ææ¨¡å‹é…ç½® (gemini-3-pro-preview with thinking)
const analysisConfig = {
  thinkingConfig: {
    thinkingLevel: 'high' as ThinkingLevel
  }
}

// å¿«é€Ÿé—®ç­”æ¨¡å‹é…ç½®
const chatConfig = {
  thinkingConfig: {
    thinkingLevel: 'low' as ThinkingLevel
  }
}

interface TokenUsage {
  promptTokens: number
  responseTokens: number
  thinkingTokens: number
  totalTokens: number
}

async function logTokenUsage(model: string, usage: TokenUsage) {
  console.log(`\nğŸ“Š ====== Token Usage [${model}] ======`)
  console.log(`   ğŸ“¥ Prompt tokens:   ${usage.promptTokens}`)
  console.log(`   ğŸ“¤ Response tokens: ${usage.responseTokens}`)
  console.log(`   ğŸ§  Thinking tokens: ${usage.thinkingTokens}`)
  console.log(`   ğŸ“ˆ Total tokens:    ${usage.totalTokens}`)
  console.log(`=====================================\n`)
  // ä¿å­˜æ•°æ®
  
}

export async function generateAnalysis(prompt: string): Promise<string> {
  // console.log('\nğŸ¤– [Analysis] Gemini API Request')
  // console.log('ğŸ“‹ Prompt:\n', prompt)
  
  const response = await getAI().models.generateContentStream({
    model: 'gemini-3-pro-preview',
    config: analysisConfig,
    contents: [{
      role: 'user',
      parts: [{ text: prompt }]
    }]
  })

  let result = ''
  let usageMetadata: unknown = null
  
  for await (const chunk of response) {
    result += chunk.text || ''
    if (chunk.usageMetadata) {
      usageMetadata = chunk.usageMetadata
    }
  }
  
  console.log('ğŸ“¤ Response:\n', result)
  
  // æ‰“å°tokenæ¶ˆè€—
  if (usageMetadata) {
    const meta = usageMetadata as Record<string, number>
    logTokenUsage('gemini-3-pro', {
      promptTokens: meta.promptTokenCount || 0,
      responseTokens: meta.candidatesTokenCount || 0,
      thinkingTokens: meta.thoughtsTokenCount || 0,
      totalTokens: meta.totalTokenCount || 0
    })
  }
  
  return result
}

export async function generateChat(prompt: string): Promise<string> {
  console.log('\nğŸ’¬ [Chat] Gemini API Request')
  console.log('ğŸ“‹ Prompt:\n', prompt)
  
  const response = await getAI().models.generateContentStream({
    model: 'gemini-2.5-flash',
    contents: [{
      role: 'user',
      parts: [{ text: prompt }]
    }]
  })

  let result = ''
  let usageMetadata: unknown = null
  
  for await (const chunk of response) {
    result += chunk.text || ''
    if (chunk.usageMetadata) {
      usageMetadata = chunk.usageMetadata
    }
  }
  
  // console.log('ğŸ“¤ Response:\n', result)
  
  // æ‰“å°tokenæ¶ˆè€—
  if (usageMetadata) {
    const meta = usageMetadata as Record<string, number>
    logTokenUsage('gemini-2.5-flash', {
      promptTokens: meta.promptTokenCount || 0,
      responseTokens: meta.candidatesTokenCount || 0,
      thinkingTokens: meta.thoughtsTokenCount || 0,
      totalTokens: meta.totalTokenCount || 0
    })
  }
  
  return result
}

export function buildNewsContext(articles: unknown[]): string {
  // å‹ç¼©JSONï¼Œæ— ç©ºæ ¼æ— æ¢è¡Œ
  return JSON.stringify(articles)
}
